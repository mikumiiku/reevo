# HUXLEY-GÖDEL MACHINE: HUMAN-LEVEL CODING AGENT DEVELOPMENT BY AN APPROXIMATION OF THE OPTIMAL SELF-IMPROVING MACHINE

Wenyi Wang* Piotr Piekos* Li Nanbo Firas Laakom Yimeng Chen Mateusz Ostaszewski Mingchen Zhuge Jürgen Schmidhuber

{wenyi.wang, piotr.piekos, nanbo.li, firas.laakom, yimeng.chen, mateusz.ostaszewski, mingchen.zhuge, juergen.schmidhuber}@kaust.edu.sa King Abdullah University of Science and Technology (KAUST) Thuwal, Saudi Arabia

# ABSTRACT

Recent studies operationalize self-improvement through coding agents that edit their own codebases. They grow a tree of self-modifications through expansion strategies that favor higher software engineering benchmark performance, assuming that this implies more promising subsequent self-modifications. However, we identify a mismatch between the agent's self-improvement potential (metaproductivity) and its coding benchmark performance, namely the Metaproductivity-Performance Mismatch. Inspired by Huxley's concept of clade, we propose a metric (CMP) that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for self-improvement. We show that, in our self-improving coding agent development setting, access to the true CMP is sufficient to simulate how the Gödel Machine would behave under certain assumptions. We introduce the Huxley-Gödel Machine (HGM), which, by estimating CMP and using it as guidance, searches the tree of self-modifications. On SWEBench Verified and Polyglot, HGM outperforms prior self-improving coding agent development methods while using fewer allocated CPU hours. Last but not least, HGM demonstrates strong transfer to other coding datasets and LLMs. The agent optimized by HGM on SWEBench Verified with GPT-5-mini and evaluated on SWEBench Lite with GPT-5 achieves human-level performance, matching the best officially checked results of human-engineered coding agents. Our code is publicly available at https://github.com/metauto-ai/HGM.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-20/2d749d4c-e09b-4a54-b7b1-258cb9c327fc/f17d4fd50535ae82d007653b1417f09c9e2074712032a5b0f3c0241ec0e3b186.jpg)

# 1 INTRODUCTION

Processes of self-modification drive the growth of complex systems, from biological evolution (Hendrikse et al., 2007; Dawkins, 2019) to cultural and scientific innovation (Good, 1966; Hall, 2007). These general ideas have been instantiated in concrete algorithms for self-improving agents (Schmidhuber, 1987; 2003; Nivel et al., 2013; Everitt et al., 2016), demonstrating how abstract principles of self-modification can be translated into operational mechanisms. Unlike static systems constrained by fixed architectures, such agents can incrementally modify their own self-modification mechanisms and learning strategies, reusing newly gained abilities to fuel subsequent improvements. This capacity fosters continual adaptation, reduces reliance on human intervention, and enables problem-solving capabilities that cannot be fully anticipated at design time.

A central challenge is how to decide which self-modifications to accept. The Gödel machine (Schmidhuber, 2003) (GM) offers a theoretically optimal answer: accept only modifications that provably increase the expected long-term utility. While this provides a sound blueprint, its reliance on formal proofs makes it practically challenging. Recent implementations instead rely on coding agents that edit their own codebases and favor self-modifications from agents with higher benchmark performance (Robeyns et al., 2025; Zhang et al., 2025a). Yet, as illustrated in Figure 1 (left), this heuristic can be misleading: a high-scoring agent may produce unproductive descendants, while a lower-scoring one seeds lineages that achieve greater long-term gains. We term this phenomenon the Metaproductivity-Performance Mismatch.

To address this mismatch, we introduce clade-level metaproductivity (CMP), inspired by Huxley's notion of clades as lineages of common ancestry (Huxley, 1957). CMP quantifies the productivity of a clade by aggregating the success of an agent's descendants rather than relying solely on its immediate benchmark score. Furthermore, we show in Theorem 1 that in our self-improving coding agent development setting (Assumption 1, which includes the assumption that the only quality of the self-improvement process is the evaluation score of the final agent and that the evaluation is conducted with repeatable trials), having access to the true CMP oracle suffices to imitate the Gödel Machine.

This insight motivates our proposed algorithm, the Huxley-Gödel Machine (HGM), which approximates GM-style self-improvement by estimating CMP from clade-aggregated descendant outcomes and selecting nodes to expand via Thompson sampling. Furthermore, by leveraging a more reliable estimate, we adaptively decouple expansion from evaluation, leading to asynchronous execution for efficient parallelism.

![](https://cdn-mineru.openxlab.org.cn/result/2025-11-20/2d749d4c-e09b-4a54-b7b1-258cb9c327fc/7ec4ab03ffa407e7b7480ae0168562ae07eaa117ab8f01c52a6d71fab3f6d2dd.jpg)  
Figure 1: (Left) Weak correlation between the guidance metrics of other methods (based on benchmark performance) and long-term self improvement; HGM mitigates this mismatch by leveraging clade-level metaproductivity. (Right) On SWE-bench Verified, HGM achieves higher accuracy with 2.38 time less allocated CPU-hours. Together, the results indicate the practical advantage of approximating Gödel Machines with long-term self-improvement estimates. Note that SICA encountered repeated errors after consuming  $45\%$  of its budget, preventing any further self-modifications.

Empirically, HGM better aligns with long-run agent productivity than benchmark-driven baselines, as shown in Figure 1 (left). On SWE-bench Verified (Jimenez et al., 2024; Chowdhury et al., 2024) and Polyglot (Gauthier, 2024), HGM consistently outperforms Darwin Gödel Machine (DGM) (Zhang et al., 2025a) and Self-Improving Coding Agent (SICA) (Robeyns et al., 2025). Remarkably, one agent found by HGM surpasses SWE-agent (Yang et al., 2024), the highest-scoring human-engineered coding agent with officially checked results, on SWE-bench Lite (Jimenez et al., 2024), when both use the GPT-5-mini backbone under matched budgets. The HGM-discovered agent transfers robustly when evaluated under a shift that is simultaneous in both the dataset and the model. Although optimized on SWE-bench Verified with GPT-5-mini, when tested on SWE-bench Lite with the GPT-5 backbone, it achieves performance on par with the best officially verified human-engineered coding agents.

To summarize, our contributions are as follows:

- We analytically define the Clade-Metaproductivity (CMP) function as a measure of agents' self-improving ability and show that in a self-improving coding agent development setting (Assumption 1), access to a CMP oracle suffices to reproduce the Gödel Machine's acceptance mechanism. (Theorem 1).  
- We empirically observe that immediate benchmark performance is an unreliable predictor of CMP and show that our CMP estimator aligns better.  
- Using our CMP estimator, we propose the Huxley-Gödel Machine (HGM), which approximates the Gödel Machine in a coding agent setting from partial evaluations and guides the expansion via Thompson sampling with adaptive scheduling.  
- We empirically validate HGM on SWE-bench Verified and Polyglot, demonstrating higher-quality optimized agents compared to previous self-improving methods, even though they were discovered within substantially smaller allocated CPU-hours. Furthermore, HGM achieves human-level coding agent design on SWE-bench Lite by optimizing on SWE-bench Verified.

# 2 SELF-IMPROVEMENT AS TREE-SEARCH

Both the Darwin Gödel Machine (DGM) and the Self-Improving Coding Agent (SICA) belong to the class of self-referential AI (Schmidhuber, 1987; 2003). In particular, in DGM and SICA, agents modify themselves to generate new agents, each empirically validated on downstream tasks.

In this paper, we formalize this self-improvement process as an iterative tree-search problem, where the goal is to discover an agent that maximizes performance across multiple downstream tasks. Concretely, starting from an initial agent as the root, a tree-search policy incrementally grows the tree of self-modified agents. At each iteration, the policy either selects an agent (a node in the tree) to expand by producing a child agent (a self-modified version of the selected agent) or selects an agent to undergo additional evaluation on downstream tasks.

Formally, let  $\mathcal{T}_t$  denote the archive of our agents at iteration  $t$ . In this paper, the archive is always represented as a tree of evolved agents, and we use the terms archives and trees interchangeably.  $\mathcal{T}_0 = \{a_0\}$  is initialized as a single-node tree with a fixed initial agent. At each iteration  $t$ , the policy selects actions  $a_{t+1} \sim \pi(\cdot \mid \mathcal{T}_t)$ , where  $\pi$  is a policy over actions  $\mathcal{A}_t = \mathcal{M}_t \cup V_t$ ,  $\mathcal{M}_t = \{m_a : a \in \mathcal{T}_t\}$  are agent modifications, and  $V_t = \{v_a : a \in \mathcal{T}_t\}$  denotes evaluations. Action  $m_a$  instructs the agent  $a$  to produce a self-modification that is added as a child of  $a$  to the tree, and  $v_a$  selects agent  $a$  from the tree for an additional evaluation on one more downstream task. After exhausting the computational budget, the policy selects a final agent ( $a_{\text{final}} = \arg \max_{a \in \mathcal{T}} Score_\pi(a) \in \mathcal{T}_B$  where  $B$  is the termination iteration and Score is part of the policy) from the final tree as the returned agent. The objective is to optimize  $J(\pi) = \mathbb{E}[U(a_{\text{final}})]$ , where  $U$  is a utility function that measures the performance of downstream tasks. In this work, we consider  $U$  the average of binary success indicators across all downstream tasks.  $\pi$  denotes an algorithm, with DGM, SICA, and our proposed HGM representing concrete instances.

Compound Policy. At each step of self-improvement, the system faces a compound decision: whether to expand the tree by generating new agents or to evaluate existing ones. This decision naturally decomposes into three sub-policies: (i) a selection policy that chooses between expansion and evaluation, (ii) an expansion policy that determines which parent to modify, and (iii) an evaluation

policy that selects an agent to test. Prior approaches, such as SICA and DGM, conflate these choices. They always expand a parent, create a child, and immediately evaluate that child on multiple tasks. This fixed sequence restricts flexibility: once a new agent is generated, it monopolizes evaluations, even if older agents appear more promising. For instance, an agent that fails nine tasks in a row continues to consume evaluations, while an older agent with partial successes is ignored.

HGM breaks this rigidity by decoupling expansion from evaluation. At each step, it adaptively decides whether to generate a new agent or to further probe an existing one, and evaluations are always at the granularity of a single agent-task pair. This finer control enables early stopping on unpromising agents. Table 5 summarizes how SICA, DGM, and HGM instantiate these sub-policies.

# 3 HUXLEY-GÖDEL MACHINE

In this section, we introduce the Huxley-Gödel Machine (HGM), a self-improving machine that approximates Gödel Machine by using clade-level statistics. At the core of HGM lies the notion of metaproductivity—a measure of an agent's ability to improve its self-improvement skills, which leads to better downstream performance of distant future agents.

In Section 3.1, we state the original Gödel machine and describe how our scope is specialized with respect to the general setup of the Gödel machine. In Section 3.2, we introduce two metrics of metaproductivity: Global metaproductivity (GMP), which captures how evolving a given agent increases the metaproductivity of the entire agent tree. This measure of metaproductivity is general and difficult to operationalize or estimate. We instead introduce clade-metaproductivity (CMP), which measures how promising the evolutions starting from a given agent (its clade) are. In Theorem 1, we show that access to true CMP is sufficient to implement a Gödel Machine applied to the coding agent development setting (Assumption 1). Following on that, in Section 3.3, we introduce the Huxley-Gödel Machine (HGM), which guides the self-improvement search with Thompson Sampling based on the estimate of CMP.

# 3.1 GÖDEL MACHINE IN THE SELF-IMPROVING CODING AGENT DEVELOPMENT SETTING

The original Gödel Machine is a general task solver that, in principle, can optimally make any provable self-improvements in any computable environment with respect to a given objective (Schmidhuber, 2003). It achieves this by running a proof searcher, continually looking for formal proofs that some modification of its own code will yield higher expected utility. Once such a proof is found, the modification is executed and permanently alters the machine. Importantly, the theoretical analysis of the Gödel Machine explicitly accounts for the fact that the agent has only a single life (with no repeatable trials) and that proving a self-improvement consumes real time and resources, which could be used for gathering the reward.

In contrast, this paper focuses on problems in a setting tailored to a particular kind of self-improving coding agent development that follows Assumption 1. Specifically, our problem assumes that the only objective is the utility of the agent selected at the end of the agent development process. Moreover, in our problem, the tests used to evaluate coding agents are conducted in a repeatable manner. This means that the testing environment is reset for each test, and prior evaluations do not influence later test results. Furthermore, each self-modification reduces the remaining budget by exactly one unit. For the theoretical analysis, we also assume that the only operation that incurs a time cost is the self-modification itself.

Within this framework designed for self-improving coding agent development, the Gödel Machine is an optimal agent operating in a POMDP where the policy observes only a parent  $a_{\mathrm{parent}}$ , its child  $a_{\mathrm{child}}$ , and the remaining budget  $b$ , and then chooses to accept or reject the child. At termination,  $Score_{\pi}$  selects either the final parent or child as output. Full description of the POMDP can be found in the Appendix A. We clarify the additional assumptions in our setup in comparison to the general Gödel Machine in Assumption 1.

Assumption 1. For the theoretical analysis of Gödel Machine applied to self-improving coding agents, we make the following additional assumptions in comparison to the setup from the original Gödel Machine:

- The policy objective function is defined as a function of only the final agent, with no other rewards received before termination;  
- The agent's utility is measured by its performance on evaluation tasks, under the assumption of repeatable trials: for any agent-task pair, the expected outcome is independent of evaluation time or prior events.  
- The proofs of Gödel Machines do not consume budget;  
- And each self-modification costs exactly one unit of the budget.

# 3.2 METAPPRODUCTIVITY AND CLADE-METAPPRODUCTIVITY

The global metaproductivity measures the impact of self-improvement on the entire current tree archive of all agents. Given a policy  $\pi$ , to quantify the quality of how an agent's self-modification influences the performance of the system, we define the notion of global metaproductivity (GMP):

$$
\operatorname {G M P} _ {\pi} (\mathcal {T}, a) = \mathbb {E} _ {\mathcal {T} _ {B} \sim p _ {\pi} (\cdot | \mathcal {T}, a)} \left[ U \left(\operatorname {a r g m a x} _ {a ^ {\prime} \in \mathcal {T} _ {B}} S c o r e _ {\pi} (a ^ {\prime})\right) \right],
$$

where  $\mathcal{T}$  is a tree of agents and  $a\in \mathcal{T}$ . Score $_{\pi}$  is the function that scores the agents for the final selection. The policy  $\pi$  unrolls the trajectory until the end of the episode with policy  $\pi$  and produces a final archive of agents  $\mathcal{T}_B$ . The distribution of the trajectory is given by  $p_{\pi}$ .

GMP directly corresponds to the Q-value function in reinforcement learning, with the state phrased as the archive of agents and the action being the selected agent to expand. The GMP value of a node measures how well (in expectation) the final agent obtained from the search process will perform. GMP measures the long-term potential of self-improvement for the entire tree, which also includes modifications that improve self-improvement itself, and so on. An algorithm might, at the beginning, focus on improving the ability to self-improve while neglecting direct benchmark abilities, only to later focus on them. This is a principal meta-learning behavior that is captured in the original Gödel Machine (Schmidhuber, 2003). The objective of designing a policy for self-improvement (Section 2) is equivalent to optimizing  $\mathrm{GMP}(\{a_0\}, a_0)$ .

While GMP captures the full global potential of a policy with access to the history of agents, its scope is too broad for practical conceptualization, since, in principle, the self-modifications of an agent can influence the expected utility of a parent agent by introducing new information. The Gödel Machine achieves global-optimality by deciding whether to accept or reject self-modification, focusing on the provable potential subsequent self-improvements. Motivated by this observation, we define a localized variant of GMP that focuses on the subtree rooted at a given agent, i.e., its clade. We refer to this quantity as Clade-Metaproductivity (CMP):

$$
\begin{array}{l} \operatorname {C M P} _ {\pi} (\mathcal {T}, a) = \mathbb {E} _ {\mathcal {T} _ {B} \sim p _ {\pi} (\cdot | \mathcal {T}, a)} \left[ U \left(\operatorname {a r g m a x} _ {a ^ {\prime} \in C \left(\mathcal {T} _ {B}, a\right)} S c o r e _ {\pi} \left(a ^ {\prime}\right)\right) \right] \\ = \mathbb {E} _ {\mathcal {T} _ {B} \sim p _ {\pi} (\cdot | \mathcal {T}, a)} \left[ \max  _ {a ^ {\prime} \in C (\mathcal {T} _ {B}, a)} U (a ^ {\prime}) \right] \quad \text {(i f S c o r e = U)}, \\ \end{array}
$$

where  $C(\mathcal{T}_B, a)$  is the clade (i.e., the subtree with  $a$  as the root) of the node  $a$  in the Tree  $\mathcal{T}_B$  and Score is the final agent selection metric. CMP contains the non-greedy information about the future evolution of self-improving agents, therefore guiding good strategies for self-improvement aimed also at the improvement of self-improvement itself. Furthermore, we show the crucial relation of CMP to the Gödel Machine.

# Theorem 1. Under Assumption 1, access to the CMP oracle is sufficient to implement the Gödel Machine.

The proof is available in the App. A. This observation motivates us to introduce the estimate of CMP and use this as guidance in our algorithm. By estimating CMP, HGM approximates the Gödel Machine. We describe our algorithm fully in the next section.

# 3.3 ALGORITHM

Existing methods use benchmark performance on coding tasks as a guiding metric, treating task success as an indicator of self-improvement potential. This assumption is overly greedy: it evaluates only the immediate utility of a modification while ignoring its downstream consequences for future self-modifications. We refer to this gap as the Metaproductivity-Performance Mismatch: the divergence between short-term task performance and the long-term capacity for self-improvement, as measured by CMP. Empirical evidence shows that this mismatch occurs in practice (see Section 4.1.) We aim to model long-term, global dependencies by deriving our estimator of CMP. Specifically, we define HGM by stating its three subpolicies.

Expansion Policy. The core of the HGM algorithm is its selection criterion for expansion. HGM aims to estimate Clade-Metaproductivity with the motivation that, in our setting, under Assumption 1, the true CMP as the criterion would produce the Gödel Machine due to Theorem 1. In this sense, HGM approximates Gödel Machine, the optimal self-improving machine. This is in contrast to the currently used greedy selection criteria based on performance metrics, which ignore the potential of the model to improve its self-improving abilities.

We estimate CMP using the weighted average of agents' empirical performance in the clade. HGM is designed to promote higher weights to higher utility agents. See below for how our evaluation policy induces this weighting strategy. Formally, let us assume a fixed archive of agents  $\mathcal{T}_t$ ,  $n_{\mathrm{success}}(a)$  be the number of passed tests of  $a$ , and  $n_{\mathrm{failure}}(a)$  be the number of failed tests of  $a$ . Then

$$
n _ {\text {s u c c e s s}} ^ {C} (a) = \sum_ {a ^ {\prime} \in C (a)} n _ {\text {s u c c e s s}} \left(a ^ {\prime}\right) \quad \text {a n d} \quad n _ {\text {f a i l u r e}} ^ {C} (a) = \sum_ {a ^ {\prime} \in C (a)} n _ {\text {f a i l u r e}} \left(a ^ {\prime}\right),
$$

where  $C(a)$  is the clade of  $a$  in  $\mathcal{T}_t$ . We define our Clade-Metaproductivity estimator as

$$
\widehat {\operatorname {C M P}} (a) = \frac {n _ {\text {s u c c e s s}} ^ {C} (a)}{n _ {\text {s u c c e s s}} ^ {C} (a) + n _ {\text {f a i l u r e}} ^ {C} (a)}.
$$

Evaluating productivity at the level of entire clades rather than individual agents offers several key advantages. It aligns better with the goal of self-improvement, as a modest ancestor can still be highly valuable if its descendants consistently advance, while stagnant lineages are deprioritized. At the same time, aggregating evidence across a clade yields more statistically robust estimates than single-node outcomes by using information from more samples. This is particularly important when evaluations are costly and benchmarks are only partially observed.

$\widehat{\mathrm{CMP}}(a)$  can be viewed as a weighted sum over the empirical means of agents in  $C(a)$ , with the weight for an agent being the number of task evaluations it has performed. Furthermore, we design our evaluation selection in such a way that it selects highly performing agents, which creates a selection of a soft maximum in the clade.

After calculating the CMP estimates, the HGM probabilistically approximates the selection of the highest scoring agent using Thompson Sampling—a standard method in the bandit literature for smoothly maximizing the decision criterion (Agrawal & Goyal, 2012; Chapelle & Li, 2011). We will refer to  $a \sim TS(\{(n_s, n_f) | n \in \mathcal{T}_t\})$  as the agent sampled from the Thompson-Sampling process with parameters  $n_s$  (number of successes) and  $n_f$  (number of failures). Given that the search problem has a known budget, our algorithm introduces an exploration-exploitation scheduler  $\tau$  that is monotonically increasing with respect to the current time  $t$ , encouraging exploration in the early stages and polarization of the sampling distribution as it approaches the end. Formally, we select the agent to expand  $a^*$  as

$$
a ^ {*} \sim T S (\left\{\left(\tau \left(1 + n _ {\text {s u c c e s s}} ^ {C} (a)\right), \tau \left(1 + n _ {\text {f a i l u r e}} ^ {C} (a)\right)\right) \mid a \in \mathcal {T} _ {t} \right\}).
$$

Evaluation Policy. As stated in the expansion policy, we design our evaluation policy to prioritize agents with a higher evaluation score to induce the selection of the maximum over the clade. Formally, the agent to evaluate  $a^*$  is sampled from the Thompson Sampling process with

$$
a ^ {*} \sim T S (\left\{\left(\tau (1 + n _ {\text {s u c c e s s}} (a)), \tau (1 + n _ {\text {f a i l u r e}} (a))\right) \mid a \in \mathcal {T} _ {t} \right\}.
$$

Selection Policy. Finally, our agent has to choose between expansion and evaluation. At each iteration, the algorithm first selects whether to evaluate or expand. Previous methods have evaluated newly created agents directly after their creation. Our novel estimation of agent self-improving quality has the additional benefit of collecting more samples more quickly (because it has samples from the entire clade). This enables more fine-grained control over when to evaluate and when to create a new agent for better efficacy. Therefore, we decouple evaluation from expansion and treat them as separate steps.

To decide how and when to evaluate or expand agents, we draw inspiration from the infinite-armed bandit literature. Infinite-armed bandit problems capture the tension between repeatedly sampling known options to reduce uncertainty about promising arms and exploring new options that have the potential to perform better. This perspective provides a natural lens for our setting, where evaluations correspond to sampling existing arms, and expansions correspond to introducing new ones. In this work, we follow the strategy of UCB-Air (Wang et al., 2008), which adds arms when the number of evaluations  $N^{\alpha} \geq m$  for some  $\alpha \in [0,1]$ , where  $m$  is the number of existing arms. In our case, arms correspond to the agents; hence, we decide to expand at time  $t$  if  $N_t^\alpha \geq |\mathcal{T}_t|$ .

Final Agent Selection Strategy. HGM iteratively executes the structured policy defined by our selection policy, expansion policy, and evaluation policy. When the computational budget exceeds, it returns the agent with the highest  $\epsilon$  percentile of the utility posterior in the final tree for some hyperparameter  $\epsilon$ , namely the best-belief agent. Formally, a best-belief agent is defined as

$$
\operatorname {a r g m a x} _ {a \in \mathcal {T} _ {B}} I _ {\epsilon} (1 + n _ {\text {s u c c e s s} (a)}, 1 + n _ {\text {f a i l u r e}} (a)),
$$

where  $I$  is the regularized incomplete beta function. See Algorithm1 in Appendix B for the detailed procedure of HGM.

Asynchronous Implementation. As an additional benefit of decoupling the policy, we introduce asynchronous execution of evaluation and expansion. Since the execution of coding agents generally requires querying large language models multiple times, the computation time can be lengthy. To boost our algorithm, we propose the asynchronous HGM algorithm (HGM Async), which utilizes all available computational power until the computational budget is exceeded. HGM Async concurrently executes an iteration of the process on each available CPU. Once one iteration finishes, a new iteration immediately starts. It uses the most recent data, with one exception, and updates the data once it is finished. The exception is that one needs to take all running expansions and explorations into consideration when executing the selection strategy. See experimental results 2 for the runtime comparison with DGM and SICA.

# 4 EXPERIMENTAL RESULTS

We evaluate HGM on challenging software engineering tasks to assess three core aspects: 1) the fidelity of HGM's CMP estimation (Sec. 4.1), 2) its capability for self-improvement with HGM compared with DGM and SICA (Sec. 4.2), and 3) the effectiveness in automatic agent design through evolutionary processes, benchmarked against a leading human design up to date $^{1}$  (Sec. 4.3). We conducted our experiments on the SWE-bench Verified (SWE-Verified) and SWE-bench Lite (SWE-Lite) variants, as well as the Polyglot problems, both of which consist of coding challenges and are widely used for coding agent evaluation (Xia et al., 2025; Zhang et al., 2024; 2025b). We follow DGM's evaluation setting for Polyglot problems, where agents have no access to private test cases or test results. For budget considerations, in addition to the full datasets, we use 60-task subsets (SWE-Verified-60), derived from the first two stages of DGM's progressive evaluation. In all experiments, we employ HGM with an exploration-exploitation scheduler  $\frac{B}{b}$ , where  $b$  is the remaining budget,  $\epsilon = 1$ , and  $\alpha = 0.6$ . All experiments involving HGM use the HGM-Async algorithm. We apply an identical initial agent when compared to DGM and SICA, which is adopted from the official implementation of DGM. See Appendix C.1 for a detailed description of the initial agents used in different experiments.

Table 1: Clade-Metaproductivity: Empirical vs. Estimation Correlation. We report the Pearson correlations between the empirical CMPs and the estimates from DGM, SICA, and HGM on SWE-Verified-60 and Polyglot. For the weighted correlations, each prediction is weighted by the number of evaluations it has accessed.  

<table><tr><td rowspan="2">Estimates</td><td colspan="2">SWE-Verified-60</td><td colspan="2">Polyglot</td></tr><tr><td>Weighted</td><td>Un-weighted</td><td>Weighted</td><td>Un-weighted</td></tr><tr><td>SICA</td><td>0.444</td><td>0.444</td><td>0.274</td><td>0.274</td></tr><tr><td>DGM</td><td>0.285</td><td>0.406</td><td>0.383</td><td>0.357</td></tr><tr><td>HGM (Ours)</td><td>0.778</td><td>0.512</td><td>0.626</td><td>0.873</td></tr></table>

# 4.1 METAPRODUCTIVITY-PERFORMANCE MISMATCH

The experiments in this section are designed to serve two purposes: (i) to provide evidence of the Metaproductivity-Performance Misalignment (MPM) issue; and (ii) to assess whether the CMP of HGM is a more reliable CMP estimator than the utility measures adopted by DGM and SICA. To reveal the misalignment, we compute the correlation between their selection criterion and empirical CMP. To obtain empirical  $\mathrm{CMP}s$ , we analyze the expanded search tree after each method has completed its run. For every node in the tree, we define its empirical CMP as the maximum empirical mean of the task performance achieved within its clade, with the root of this clade excluded. This construction ensures that empirical CMP captures the self-improvement ability of a node. We exclude the root of a clade to avoid the circular use of the target in the estimators. For HGM, the CMP is defined as a function over the clade of a node; a critical adjustment is required to avoid target leakage. Specifically, we exclude the evaluations that are most directly related to the target: the root of the clade (an ancestor of the target) and the subtree rooted at a direct child of the evaluated node that contains the empirical maximum, thereby ensuring a fair comparison (see Appendix D for detailed computation). We report both the correlation coefficient weighted by the number of evaluations used in prediction, as well as the unweighted correlation. We conducted our experiments on the SWE-Verified-60 and Polyglot datasets.

Results & Discussion. Table 1 summarizes the correlations between the three estimators and the empirical CMPs (the targets). We first observe that the SICA and DGM estimators achieve positive Pearson correlation coefficients: 0.444 and 0.285 on SWE-Verified-60, and 0.274 and 0.383 on Polyglot, respectively, suggesting weak alignments, i.e., MPM. In comparison, HGM's estimator,  $\widehat{\mathsf{CMP}}$ , achieves substantially stronger weighted correlations of 0.778 and 0.626 on SWE-Verified-60 and Polyglot, respectively, as well as unweighted correlations of 0.512 and 0.8783, surpassing SICA and DGM by wide margins. These results provide strong indications that HGM, equipped with  $\widehat{\mathsf{CMP}}$ , offers a more reliable estimate of metaproductivity and effectively mitigates the MPM issues inherent to SICA and DGM.

# 4.2 EVALUATING HGM SELF-IMPROVING CAPABILITY

To validate our hypothesis that our CMP estimator better predicts future self-improvement and, hence, leads to more effective self-modifications, we evaluate HGM against two state-of-the-art self-improving coding agent methods: DGM and SICA. We conduct controlled experiments under the same setup as DGM, with a budget-friendly modification applied consistently to both HGM and all baselines to ensure fairness, i.e., we adopt more cost-efficient backbone LLMs (GPT-5 for expansion and GPT-5-mini for evaluation for SWE-Verified; Qwen3-Coder-480B-A35B-Instruct for expansion and Qwen3-Coder-30B-A3B-Instruct for evaluation for Polyglot). For all methods, we start with the same initial ancestor, which achieves  $40\%$  and  $20.3\%$  on the SWE-Verified-60 and Polyglot, respectively. We compare the task performance of their best-belief final agents after a maximum allowance of 800 benchmark task evaluations, selected in DGM and SICA using empirical means. In addition, our asynchronous parallelization of expansion and evaluation enables self-improvement to consume fewer allocated CPU-hours than DGM and SICA (see Sec. 3.3). We verify this and report the allocated CPU-hours required for 800 evaluations.

Table 2: Self-Improving Capability Comparison. We report the task performance (in accuracy) of each method's best-belief agent and the allocated CPU-hours time required for 800 evaluations. Superscripted accuracies with "+" indicate performance gains over their respective initial agents.  

<table><tr><td rowspan="2">Best-belief Agent of</td><td colspan="2">SWE-Verified-60</td><td colspan="2">Polyglot</td></tr><tr><td>Acc. (%)↑</td><td>Time (hours)↓</td><td>Acc. (%)↑</td><td>Time (hours)↓</td></tr><tr><td>SICA</td><td>50.0+10</td><td>infinite loop</td><td>25.4+5.1</td><td>572</td></tr><tr><td>DGM</td><td>53.3+13.3</td><td>1231</td><td>27.1+6.8</td><td>2385</td></tr><tr><td>HGM (Ours)</td><td>56.7+16.7</td><td>517</td><td>30.5+10.2</td><td>347</td></tr></table>

Results & Discussion. We summarize the comparison results in Table 2. Across both SWE-Verified-60 and Polyglot, all three methods successfully perform agent discovery by optimizing the initial agent through self-improvement. However, HGM's best-belief agent demonstrates not only the highest task performance— $56.7\%$  on SWE-Verified-60 and  $30.5\%$  on Polyglot—but also the best efficiency, requiring the fewest allocated CPU-hours for 800 evaluations: 6.86 times faster than DGM and 1.65 times faster than SICA on Polyglot, and 2.38 times faster than DGM on SWE-Verified-60. Notably, on SWE-Verified-60, SICA repeatedly encounters "query length out-of-LLM-context-window" during self-improvement processes after 360 evaluations. Despite this, the Polyglot results validate our hypothesis regarding HGM's runtime advantage over the baselines. In conclusion, HGM, equipped with a better utility estimator and asynchronous expansion-evaluation iterations, establishes itself as a more effective self-improving mechanism compared to DGM and SICA.

# 4.3 HGM VS. HUMANS: ON CODING AGENTS DESIGN

To gain a better understanding of its potential, we extend our evaluation of HGM by benchmarking it against the best human performance in coding agent design on SWE-Lite. We consider two settings: 1) optimization on full SWE-Verified and 2) generalization to SWE-Lite.

# 4.3.1 OPTIMIZATION ON FULL SWE-BENCH VERIFIED

In this experiment, rather than using the SWE-Verified-60, we scaled the HGM evaluation to the full SWE-Bench Verified benchmark (500 coding challenges) with an increased number of HGM iterations (8000 evaluations). We further adjusted the initial agent so that it yields an improved accuracy of  $53.2\%$ . See Appendix C.1 for the adjustment details. Notably, this stronger starting point underscores the difficulty of further improvement due to a higher baseline. Despite this, HGM demonstrates significant gains and strong absolute performance.

Results & Discussion. After 8000 evaluations, HGM discovered an optimized agent that solves  $61.4\%$  tasks, surpassing the best human-designed agent built on GPT-5-mini on the SWE-Verified leaderboard. This establishes our discovered agent as the top-scoring GPT-5-mini-based system, and positions it among the top-10 agents over all checked submissions, even when compared to systems built on stronger backbone models that can cost  $5 \times$  more (e.g., Claude-3.7). While higher scores on the leaderboard do not necessarily indicate superior general coding ability—since both human- and machine-designed agents may overfit to the benchmark—these results demonstrate a promising potential of HGM for competing with established human-designed baselines under identical model constraints.

# 4.3.2 GENERALIZATION TO SWE-BENCH LITE

To validate that HGM's self-evolution produces agents with stronger general coding ability, rather than merely overfitting to SWE-Verified, we evaluate the top agent discovered on SWE-Verified against unseen tasks. Specifically, we compare this agent with its initial ancestor (which achieved  $53.2\%$  on SWE-Verified) using SWE-Lite, a benchmark of 300 coding tasks, 93 of which overlap with SWE-Verified. For rigor and comparability, we report two settings: (i) a filtered setting where the 93 overlapping tasks are excluded, leaving only completely unseen tasks, and (ii) the full 300-task benchmark, identical to the standard evaluation used for human designs on the leaderboard. As

Table 3: Generalization on SWE-Lite: HGM's Best-belief SWE-Verified Agent. We report the accuracy of HGM's best-belief SWE-Verified agent on SWE-Lite under two settings: filtered (excluding tasks overlapping with SWE-Verified) and standard (the official leaderboard setting used for evaluating human-designed agents)).  

<table><tr><td>Coding Agents</td><td>SWE-Lite Filtered (%)</td><td>SWE-Lite Standard (%)</td></tr><tr><td>HGM Initial Ancestor</td><td>34.8</td><td>44.0</td></tr><tr><td>SWE-agent+GPT-5-mini</td><td>39.6</td><td>47.6</td></tr><tr><td>HGM&#x27;s Best-belief SWE-Verified Agent</td><td>40.1</td><td>49.0</td></tr></table>

Table 4: Transfer to different LLMs on SWE-Lite: HGM's Best-belief SWE-Verified Agent. Similarly, We report the accuracy of HGM's best-belief SWE-Verified (optimized with GPT-5-mini) agent on SWE-Lite (evaluated with GPT-5) under two settings: filtered (excluding tasks overlapping with SWE-Verified) and standard (the official leaderboard setting used for evaluating human-designed agents).  

<table><tr><td>Coding Agents</td><td>SWE-Lite Filtered (%)</td><td>SWE-Lite Standard (%)</td></tr><tr><td>SWE-agent (Best on the LB)</td><td>48.3</td><td>56.7</td></tr><tr><td>HGM&#x27;s Best-belief SWE-Verified Agent + GPT-5</td><td>47.8</td><td>57</td></tr></table>

of the time of writing, no checked submission using GPT-5-mini appears on the SWE-Lite leaderboard. To control for backbone differences and isolate agent design, we adapt the leading system (with checked submissions) (SWE-agent + Claude 4 Sonnet) by replacing its backbone with GPT-5-mini, yielding SWE-agent + GPT-5-mini, as an additional baseline for comparison.

Results & Discussion. We show the generalization results of HGM's best-belief SWE-Verified agent on SWE-Lite benchmark in Table 3. The best-belief HGM agent found on SWE-Verified achieves  $40.1\%$  under the filtered (completely unseen) setting and  $49.0\%$  under the standard setting. Compared to its initial ancestor  $(34.8\%)$  and  $44.0\%$ , respectively), these gains substantiate the effectiveness of HGM's self-evolution in improving general coding ability—rather than overfitting to the optimization set. Notably, the superior performance of our HGM agent achieved on the standard SWE-Lite places it firmly in second place on the SWE-Lite leaderboard among all checked submissions. Moreover, based on our local execution result of SWE-agent using the SWE-agent + Claude 4 Sonnet submission version with the same configuration, the agent optimized by HGM outperforms SWE-agent + GPT-5-mini, which achieves  $39.6\%$  (vs.  $40.1\%$  for us) on the filtered and  $47.6\%$  (vs.  $49.0\%$  for us) on the standard. This demonstrates that the edge arises not from the GPT-5-mini backbone, but from the genuine design improvements introduced by HGM evolution.

Transfer to bigger LLMs. We also examined how the discovered agent scales when paired with larger and better-performing LLMs. In particular, we replaced the GPT-5-mini backbone of HGM's Best-belief agent with the GPT-5 model to test whether the agent optimized with one LLM remains effective with another. The results indicate that the agent maintains its strong performance under this transfer. In particular, its accuracy on the SWE-Lite benchmarks is comparable to that of state-of-the-art, human-engineered coding agents reported on the leaderboard, suggesting that HGM's self-evolved design principles are reliably transferred across backbone sizes. The agent discovered by HGM with GPT outperforms all other agents on the SWE-Bench Lite leaderboard with officially checked results and is one task behind the best-performing model on our selected "SWE-bench Filtered". In Table 4, we compare it with the SWE-agent, which holds first place on the official SWE-Bench Lite leaderboard at the time of publication. This result further supports the conclusion that the improvements are due to genuine agent improvement rather than overfitting to a particular dataset or LLM.

# 5 RELATED WORKS

The general concepts of machine self-improvement were first systematically articulated by Good (1966), who described the possibility of "Intelligence Explosion" once machines acquire the capacity to design more capable successors. Early work on explicit self-improvements dates back to Schmidhuber (1987), which introduced self-referential learning mechanisms in which a system generates and evaluates modified descendant versions of itself. Follow-up work on self-improvement progressed through interaction and agentic reinforcement learning. The Success-Story Algorithm(SSA) (Schmidhuber & Zhao, 1996; Schmidhuber et al., 1997) progressively forces self-modifying policies to discover more effective self-modification strategies. Its core mechanism is based on hindsight: at each checkpoint, a sequence of self-modifications that did not yield higher long-term reward rates is systematically undone. In this way, SSA enforces continual improvement by ensuring that only those self-modifications associated with demonstrably greater reward intake per unit time are preserved. Fitness-Monotonic Execution (Kirsch & Schmidhuber, 2022a,b) reduces the outer-loop design by favoring the execution of models with higher ancestral performance. Meta-discovered update rules optimized optimizers (Metz et al., 2021) and black-box search (Lange et al., 2023). On the other hand, the Gödel Machine, a fully self-referential algorithm that rewrites its own code whenever it can prove an expected-utility improvement, provides a provably and globally optimal mechanism for self-improvement (Schmidhuber, 2003).

The rise of contemporary LLMs has created an opportunity to automate substantial aspects of software engineering. One concrete step in this direction is the development of coding agents, which extend LLMs with the ability to operate in conventional computing environments. ChatDev (Qian et al., 2023) first illustrated this idea in the context of automated bug fixing, and similar frameworks were later explored in SWE Yang et al. (2024), OpenHands (Wang et al., 2024), MetaGPT (Hong et al., 2024), and AgentLess (Xia et al., 2025).

The Self-Taught Optimizer (Zelikman et al., 2024) and Gödel Agent (Yin et al., 2024) first experimented with agents that modify their own scaffolding. Subsequently, DGM (Zhang et al., 2025a) and SICA (Robeyns et al., 2025) extend this direction by implementing self-modifying machines as full software engineering projects, where agents self-reference and modify their own repositories while validating changes through execution-grounded software engineering tasks. Both DGM and SICA, explicitly or implicitly, assume that higher software benchmark scores correspond to greater self-improvement capacity. In contrast, HGM introduces a qualitative measure of self-improvement consistent with the theoretical Gödel Machine and directs self-modifications using estimates of this measure.

The identified tree-search problem spans fixed-budget best-arm identification (BAI), Monte Carlo Tree Search, and infinite-armed bandits, introducing a distinct decision: explicit expansion actions that create new candidate leaves alongside ordinary evaluations. Fixed-budget BAI and Bayesian value-of-information methods assume a finite and known set of arms and offer guarantees for static candidates, thus not modeling the discovery of unknown arms (Audibert & Bubeck, 2010; Karnin et al., 2013; Frazier et al., 2008). Monte-Carlo Tree Search and its UCT variants (Coulom, 2006; Kocsis & Szepesvári, 2006) alternate selection, expansion, and simulation, while their backup and selection rules typically target cumulative reward rather than fixed-budget final-choice objectives under noisy, low-signal feedback, with limited guarantees for pure exploration of leaf quality (Kaufmann & Koolen, 2017). Infinite-armed bandit formulations capture the explore-discover tradeoff but typically model discoveries as i.i.d. draws from a reservoir, missing tree structure, and hierarchical dependencies (Wang et al., 2008; Bubeck et al., 2011; Carpentier & Valko, 2015).

# 6 CONCLUSION

In this work, we identify a key limitation in the search heuristics of current self-improving coding agents: Benchmark scores alone do not reliably indicate an agent's long-term potential for self-improvement, since high-scoring agents can still lead to stagnating lineages, while seemingly weaker ones may seed productive self-improvements. We refer to it as the Metaproductivity-Performance Mismatch. To address this gap, we introduce Clade-Metaproductivity (CMP), a lineage-based metric inspired by Huxley's notion of clades. We show that, under certain assumptions, when applied

to our self-improving coding agent search problem (Assumption 1), the CMP oracle is sufficient to implement the Gödel Machine (Theorem 1).  
Building on this principle, we propose the Huxley-Gödel Machine (HGM), which approximates CMP and uses it to guide expansion through Thompson sampling with adaptive scheduling. Empirically, HGM consistently produces higher quality agents than prior self-improving frameworks while also reducing wall-clock time. Notably, HGM generalizes across both dataset and model shifts, achieving human-level coding agent design performance on SWE-bench Lite with GPT-5 despite being optimized on SWE-bench Verified with GPT-5-mini.  
Taken together, these results suggest that clade-based measures of improvement potential, rather than immediate performance alone, lead to more effective forms of self-improvement. By demonstrating that clade-level evaluation can reliably guide the growth of coding agents, this work points to a new paradigm for the design of agentic improvement: one in which improvement is driven not by narrow benchmarks, but by the long-term generative potential of entire lineages. This perspective underscores the importance of systems that strengthen an agent's capacity to keep improving over time, rather than merely boosting their performance in the short term.

# ACKNOWLEDGMENT

We thank Yuhui Wang for the discussions during the early stages of this project. We gratefully acknowledge Jenny Zhang and Shengran Hu, the authors of Darwin Gödel Machine, for sharing their insights about DGM and their implementation experience. We also thank Yilan Zhang, Rui Zhang, and Lisiyu Xie for their help in designing the visualizations. The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940.
